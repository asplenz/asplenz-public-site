# [FR VERSION]

# IA Appliquée : Diagnostic Décision & Responsabilité

### Comprendre où l'IA génère de la responsabilité avant que cela ne devienne un problème.

---

## Pourquoi ce diagnostic existe

Les organisations intègrent rapidement l'IA et l'automatisation dans leur travail quotidien. Ce qui change en premier, ce n'est pas la productivité : c'est **la manière dont les décisions sont prises**.

Aujourd'hui, de nombreuses décisions sont :

* partiellement automatisées,
* assistées par l'IA,
* ou indirectement influencées par des systèmes opaques.

Pourtant, la plupart des organisations ne peuvent pas répondre clairement à ces questions :

* **Quelles décisions sont déléguées à l'IA ou à l'automatisation ?**
* **Qui en est responsable (accountable) ?**
* **Quelles preuves attestent de la manière dont elles ont été prises ?**
* **Ces décisions seraient-elles défendables lors d'un audit, d'un litige ou d'un contrôle réglementaire ?**

Pour les directions, cela crée un angle mort : **une exposition croissante au risque de responsabilité sans vision claire de l'endroit où il se situe.**

Ce travail de terrain existe pour rendre ces questions explicites avant qu'elles ne se transforment en passifs opérationnels, juridiques ou réputationnels.

---

## Objectif de l'intervention

Aider les équipes à **identifier où l'IA et l'automatisation introduisent des décisions à impact réel**, et évaluer si ces décisions sont actuellement :

* gouvernables,
* auditables,
* et prouvables dans le temps.

Cette mission n'a **pas pour but de déployer plus d'IA**. Elle vise à **être en mesure d'assumer la responsabilité des décisions assistées par l'IA**.

À l'issue de l'intervention, les équipes disposent de :

* une cartographie des décisions influencées par l'IA,
* une vision claire des écarts de responsabilité associés,
* l'identification des décisions qui seraient fragiles en cas de contestation,
* une compréhension explicite des points où des preuves plus solides sont requises.

---

## Ce sur quoi nous travaillons concrètement

### Cartographie des décisions (cœur de mission)

Nous partons du **travail réel**, pas de la technologie.

Identifier les décisions qui sont :

* automatisées,
* assistées par l'IA,
* ou influencées indirectement par des outils d'IA.

Pour chaque décision, comprendre :

* qui la déclenche,
* quels systèmes et données sont impliqués,
* quelles traces existent (ou non),
* comment la responsabilité est actuellement perçue.

Cette phase révèle souvent des angles morts dont les équipes n'avaient pas conscience.

---

### Analyse des preuves et des écarts de responsabilité

Pour les décisions identifiées, nous évaluons :

* si la décision peut être reconstruite *a posteriori*,
* s'il existe des preuves fiables des données d'entrée, du contexte, du raisonnement et de l'exécution,
* où la responsabilité est claire et où elle est fragmentée.

Nous identifions les décisions qui seraient fragiles lors :

* d'audits,
* de contrôles réglementaires,
* de litiges,
* ou de processus internes de reddition de comptes.

**C'est ici que le risque futur devient visible.**

---

### Ajustement des usages et des pratiques

Ce n'est qu'une fois la responsabilité comprise que nous abordons l'usage.

* Ajuster l'utilisation de l'IA pour réduire l'opacité décisionnelle.
* Recommander des pratiques qui améliorent la traçabilité, réduisent la délégation incontrôlée et clarifient la responsabilité.
* Lorsque les écarts de responsabilité ne peuvent être comblés par la seule pratique : recommander des outils ou configurations existants, identifier les garde-fous structurels manquants.

**L'objectif n'est pas l'optimisation : c'est la gouvernabilité.**

---

## Ce que cette intervention est, et ce qu'elle n'est pas

### C'est :

* Un travail de terrain ancré dans les opérations réelles.
* Une mission courte, ciblée et délimitée dans le temps.
* Orienté vers la responsabilité future.
* Particulièrement pertinent pour les environnements réglementés.

### Ce n'est pas :

* Une offre de conseil en IA générique.
* Un programme d'optimisation de la productivité.
* Du développement de modèles sur mesure.
* Un projet de transformation à long terme.

---

## Équipes types concernées

* Produit et Ingénierie
* Opérations et Back-office
* Marketing et Ventes (là où l'IA influence les décisions)
* Risques, Conformité et Gouvernance
* Directions cherchant à clarifier leur exposition aux risques IA

---

## Le lien avec Asplenz

Ce travail de terrain n'est **pas une activité séparée**. Il permet à Asplenz de :

* observer comment les décisions influencées par l'IA sont réellement prises,
* identifier les lacunes récurrentes en matière de responsabilité,
* ancrer le développement de nos produits dans la réalité opérationnelle,
* garantir qu'Asplenz répond à des **problèmes de responsabilité réels et non théoriques**.

C'est notre manière de rester connectés à la réalité opérationnelle.

---

## Format de l'intervention

* Missions courtes (généralement **2 à 4 semaines**).
* Périmètre clair et limité.
* Sans engagement de durée.
* Conçu pour **éclairer des décisions**, pas pour vendre de la technologie.

---

> "Les organisations ne seront pas jugées sur leur utilisation de l'IA, mais sur leur capacité à **en assumer la responsabilité**."

---

## Contact

Si vous souhaitez comprendre comment l'IA et l'automatisation façonnent déjà les décisions au sein de votre organisation et si vous êtes prêt à en assumer la responsabilité :

→ [Contactez-nous](/fr/contact)

---

# [EN VERSION]

# Applied AI — Decision & Responsibility Field Work

### Understanding where AI creates responsibility — before it becomes a problem.

---

## Why this exists

Organizations are rapidly introducing AI and automation into everyday work. What changes first is not productivity — it is **how decisions are made**.

Many decisions are now:

* partially automated,
* assisted by AI,
* or indirectly influenced by opaque systems.

Yet most organizations cannot clearly answer:

* **Which decisions are delegated to AI or automation?**
* **Who is accountable for them?**
* **What evidence exists of how they were made?**
* **Would these decisions be defensible in an audit, a dispute, or a regulatory review?**

For leadership teams, this creates a blind spot: **a growing exposure to accountability risk without a clear view of where it sits.**

This field work exists to make these questions explicit — **before they turn into operational, legal, or reputational liabilities**.

---

## Objective of the engagement

Help teams **identify where AI and automation introduce decisions with real impact**, and assess whether these decisions are currently:

* governable,
* auditable,
* and provable over time.

This engagement is **not about doing more AI**. It is about **being able to assume responsibility for AI-assisted decisions**.

At the end of the engagement, teams leave with:

* a mapped set of AI-influenced decisions,
* a clear view of associated responsibility gaps,
* identification of decisions that would be fragile if challenged,
* an explicit understanding of where stronger proof would be required.

---

## What we actually work on

### Decision mapping (core work)

We start from **real work**, not technology.

Identify decisions that are:

* automated,
* AI-assisted,
* or indirectly influenced by AI tools.

For each decision, understand:

* who triggers it,
* what data and systems are involved,
* what traces exist (or do not),
* how responsibility is currently perceived.

This phase often reveals blind spots teams were not aware of.

---

### Proof and responsibility gap analysis

For the identified decisions, we assess:

* whether the decision can be reconstructed ex post,
* whether reliable evidence exists of inputs, context, rationale, execution,
* where accountability is clear — and where it is fragmented.

We identify which decisions would be fragile in:

* audits,
* regulatory reviews,
* disputes,
* or internal accountability processes.

**This is where future risk becomes visible.**

---

### Usage and practice adjustment

Only once responsibility is understood do we address usage.

* Adjust AI usage to reduce decision opacity.
* Recommend practices that improve traceability, reduce uncontrolled delegation, clarify accountability.
* When responsibility gaps cannot be addressed through practice alone: recommend existing tools or configurations, identify where structural guardrails are missing.

**The goal is not optimization — it is governability.**

---

## What this engagement is — and is not

### This is:

* Field work grounded in real operations
* Short, focused, and intentionally time-boxed
* Oriented toward future accountability
* Particularly relevant in regulated or high-responsibility environments

### This is not:

* A generic AI consulting offer
* A productivity optimization program
* Custom AI or model development
* A long-term transformation project

---

## Typical teams involved

* Product and engineering
* Operations and back-office
* Marketing and sales (where AI influences decisions)
* Risk, compliance, and governance
* Leadership teams seeking clarity on AI exposure

---

## How this connects to Asplenz

This field work is **not a separate business**. It allows Asplenz to:

* observe how AI-influenced decisions are actually made,
* identify recurring responsibility gaps,
* ground product work in operational reality,
* ensure Asplenz addresses **real, not theoretical, accountability problems**.

Applied AI — Decision & Responsibility Field Work is how we stay connected to reality.

---

## Engagement format

* Short engagements (typically **2–4 weeks**)
* Clear and limited scope
* No lock-in
* Designed to **inform decisions**, not to sell technology

---

> "Organizations will not be judged on whether they used AI, but on whether they were able to **assume responsibility for it**."

---

## Get in touch

If you want to understand where AI and automation are already shaping decisions in your organization — and whether you are prepared to assume responsibility for them:

→ [Get in touch](/en/contact)
