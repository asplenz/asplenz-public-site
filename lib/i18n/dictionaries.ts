import type { Locale } from './config'

const dictionaries = {
  en: {
    hero: {
      headline: 'Prove what happened.',
      subheadline: 'Not what should have happened.',
      description: 'Horizon provides continuous oversight of AI decision behaviour, without influencing execution, and produces defensible evidence for individual AI decisions.',
      clarifier: 'Not monitoring. Not evaluation. Not enforcement.',
      microLine: 'Built for scrutiny, not demos.',
      primaryCTA: 'Request a briefing',
      secondaryCTA: 'Read the assurance philosophy',
    },
    problem: {
      title: 'The Problem',
      subtitle: 'The missing capability in AI systems today',
      paragraph1: 'Modern AI systems increasingly support or produce consequential decisions.\nWhen those systems are observed in operation, teams rely on dashboards, metrics, and alerts to understand behaviour in real time. This visibility is necessary, but it is not sufficient.',
      paragraph2: 'When a specific decision is examined, the question changes:\nWhat exactly happened in this case?',
      paragraph3: 'Most organizations can describe:',
      list1: [
        'how systems behave on average',
        'what controls and policies exist',
        'what current indicators show',
      ],
      paragraph4: 'They often cannot demonstrate a single decision with stable, examinable facts.\nWhen challenged, teams reconstruct context from logs, configurations, and memory.',
    },
    consequence: {
      title: 'The Consequence of Missing Proof',
      paragraph1: 'When factual proof is missing:',
      list: [
        'oversight relies on interpretation',
        'incidents trigger manual reconstruction',
        'audits depend on narrative coherence',
      ],
      paragraph2: 'A decision can appear compliant and still be indefensible if it cannot be substantiated.\nThis gap is rarely visible during normal operations.\nIt becomes critical under scrutiny.',
    },
    whatIs: {
      title: 'What Horizon Is',
      paragraph1: 'Horizon is an assurance capability designed to operate alongside AI systems without influencing their execution.\nIt provides continuous oversight of system behaviour while preserving factual evidence about individual decisions.',
      paragraph2: 'Horizon does not guide, block, or optimize decisions.\nIt does not determine whether a decision is correct or acceptable.\nIt exists to make behaviour examinable, both during operation and under later scrutiny.',
    },
    produces: {
      title: 'What Horizon Produces',
      paragraph1: 'For each AI-supported decision, Horizon ensures the existence of a decision-level assurance artefact.',
      paragraph2: 'This artefact is designed to:',
      list: [
        'anchor discussions in facts rather than interpretation',
        'remain usable beyond the operational context',
        'support independent examination',
      ],
      paragraph3: 'The artefact is present before any incident, audit, or dispute.\nEvidence that must be assembled later is no longer evidence.',
      paragraph4: 'The assurance artefact captures the factual elements necessary to reconstruct what occurred in a specific decision, without interpretation or post-processing.\nIt remains usable independently of Horizon.',
      paragraph5: 'Horizon does not assemble evidence.\nIt ensures that evidence exists.',
    },
    evidenceEval: {
      title: 'Evidence and Evaluation',
      paragraph1: 'Horizon is grounded in a strict separation.',
      paragraph2: 'Evidence describes what actually occurred.\nEvaluation provides signals to support human judgment.',
      paragraph3: 'Visibility and evaluation evolve over time.\nEvidence remains stable.',
      paragraph4: 'Horizon never replaces human judgment.\nIt ensures that judgment can rely on facts.',
      tagline: 'Signals suggest. Evidence proves.',
    },
    notIs: {
      title: 'What Horizon Is Not',
      paragraph1: 'To avoid ambiguity, Horizon is explicitly not:',
      list: [
        'a monitoring platform',
        'a performance or quality evaluator',
        'an explainability interface',
        'a runtime control or guardrail',
        'a decision engine',
        'a governance dashboard',
      ],
      paragraph2: 'Horizon does not intervene in generation.\nIt observes, preserves, and attests.',
      paragraph3: 'Horizon cannot be replicated by augmenting logs, adding explanations, or reconstructing decisions after the fact.',
    },
    relationship: {
      title: 'Relationship to Existing Tooling',
      paragraph1: 'Operational tools provide visibility, alerts, and control during execution.\nHorizon adds a different capability.',
      paragraph2: 'It ensures that what happens at runtime can later be examined as evidence, without relying on dashboards, configurations, or vendor access.',
      tagline1: 'Visibility is continuous.',
      tagline2: 'Defensibility is permanent.',
    },
    assurance: {
      title: 'Assurance Properties',
      paragraph1: 'Assurance produced through Horizon is designed to be:',
      list: [
        'scoped to individual decisions',
        'consistent over time',
        'interpretable without proprietary context',
        'independently verifiable without the Horizon platform',
        'suitable for audit, regulatory review, or legal scrutiny',
      ],
      paragraph2: 'Interpretation does not depend on configuration or tuning.',
    },
    deployment: {
      title: 'Deployment and Trust Boundaries',
      paragraph1: 'Horizon operates within the organization\'s trust boundary.\nThe organization retains control over:',
      list: [
        'its systems',
        'its data',
        'its assurance material',
      ],
      paragraph2: 'Horizon does not require ongoing access to interpret or validate evidence.\nEvidence ownership remains with the organization.',
      paragraph3: 'Horizon does not trigger mitigation.\nIt enables informed human response.',
    },
    users: {
      title: 'Intended Users',
      paragraph1: 'Horizon is designed for teams responsible for oversight and accountability when AI decisions matter:',
      list: [
        'Risk and Compliance',
        'Internal Audit',
        'AI Governance',
        'Engineering teams operating AI systems at scale',
      ],
      paragraph2: 'Horizon creates a shared factual substrate across these functions.\nIt is built for environments where decisions may be challenged.',
    },
    when: {
      title: 'When Horizon Becomes Necessary',
      paragraph1: 'Horizon is valuable before anything goes wrong.\nIt is valuable during normal operation and critical when scrutiny arises.',
      paragraph2: 'It becomes necessary when:',
      list: [
        'individual AI decisions can be challenged',
        'regulatory or legal review is expected',
        'post-incident analysis must rely on facts',
        'explanations must remain defensible over time',
      ],
      paragraph3: 'If a decision matters, proof must already exist.',
    },
    philosophy: {
      title: 'Philosophy',
      paragraph1: 'Horizon enables organizations to adopt AI systems without relying on fragile reconstructions or retrospective narratives.',
      tagline: 'Signals suggest. Evidence proves.',
    },
    cta: {
      title: 'Call to Action',
      paragraph1: 'Learn how Horizon fits into an AI assurance strategy focused on accountability under scrutiny.',
      primaryCTA: 'Request a briefing',
      secondaryCTA: 'Discuss audit and regulatory use cases',
    },
    contact: {
      title: 'Email',
      intro: 'Our email:',
      copied: 'Copied',
      copy: 'Copy email',
      mailto: 'Open in my mail client',
      note: 'You can paste this email into your mail application.',
    },
  },
  fr: {
    hero: {
      headline: 'Prouver ce qui s\'est passé.',
      subheadline: 'Pas ce qui aurait dû se passer.',
      description: 'Horizon assure une supervision continue du comportement décisionnel des systèmes d\'IA, sans influencer leur exécution, et produit des preuves défendables pour chaque décision individuelle.',
      clarifier: 'Ni monitoring. Ni évaluation. Ni enforcement.',
      microLine: 'Conçu pour le contrôle, pas pour la démonstration.',
      primaryCTA: 'Demander un briefing',
      secondaryCTA: 'Lire la philosophie d\'assurance',
    },
    problem: {
      title: 'Le problème',
      subtitle: 'La capacité manquante aujourd\'hui',
      paragraph1: 'Les systèmes d\'IA soutiennent de plus en plus des décisions à impact.\nEn exploitation, les équipes s\'appuient sur des tableaux de bord, des métriques et des alertes pour comprendre le comportement en temps réel. Cette visibilité est nécessaire, mais elle n\'est pas suffisante.',
      paragraph2: 'Lorsqu\'une décision précise est examinée, la question change :\nQue s\'est-il exactement passé dans ce cas ?',
      paragraph3: 'La plupart des organisations peuvent décrire :',
      list1: [
        'le comportement « en moyenne »',
        'les contrôles et politiques existants',
        'ce que montrent les indicateurs du moment',
      ],
      paragraph4: 'Elles ne peuvent souvent pas démontrer une décision unique à partir de faits stables et examinables.\nEn situation de contrôle, les équipes reconstruisent le contexte à partir de logs, de configurations et de mémoire.',
    },
    consequence: {
      title: 'Conséquence de l\'absence de preuve',
      paragraph1: 'Quand la preuve factuelle manque :',
      list: [
        'la supervision repose sur l\'interprétation',
        'les incidents déclenchent une reconstruction manuelle',
        'les audits dépendent de la cohérence du récit',
      ],
      paragraph2: 'Une décision peut sembler conforme tout en restant indéfendable si elle ne peut pas être étayée.\nCet écart est rarement visible en fonctionnement normal.\nIl devient critique en situation de contrôle.',
    },
    whatIs: {
      title: 'Ce qu\'est Horizon',
      paragraph1: 'Horizon est une capacité d\'assurance conçue pour fonctionner aux côtés des systèmes d\'IA sans influencer leur exécution.\nIl assure une supervision continue du comportement, tout en préservant une preuve factuelle des décisions individuelles.',
      paragraph2: 'Horizon ne guide pas, ne bloque pas et n\'optimise pas les décisions.\nIl ne détermine pas si une décision est correcte ou acceptable.\nIl existe pour rendre le comportement examinable, en exploitation comme sous contrôle ultérieur.',
    },
    produces: {
      title: 'Ce que produit Horizon',
      paragraph1: 'Pour chaque décision appuyée par l\'IA, Horizon garantit l\'existence d\'un artefact d\'assurance au niveau de la décision.',
      paragraph2: 'Cet artefact est conçu pour :',
      list: [
        'ancrer les discussions dans les faits plutôt que dans l\'interprétation',
        'rester utilisable au-delà du contexte opérationnel',
        'permettre un examen indépendant',
      ],
      paragraph3: 'L\'artefact est présent avant tout incident, audit ou litige.\nUne preuve qui doit être assemblée plus tard n\'est plus une preuve.',
      paragraph4: 'L\'artefact d\'assurance capture les éléments factuels nécessaires à la reconstitution de ce qui s\'est produit dans une décision donnée, sans interprétation ni traitement a posteriori.\nIl reste utilisable indépendamment de Horizon.',
      paragraph5: 'Horizon n\'assemble pas les preuves.\nIl garantit que les preuves existent.',
    },
    evidenceEval: {
      title: 'Preuve et évaluation',
      paragraph1: 'Horizon repose sur une séparation stricte.',
      paragraph2: 'La preuve décrit ce qui s\'est réellement produit.\nL\'évaluation fournit des signaux pour soutenir le jugement humain.',
      paragraph3: 'La visibilité et l\'évaluation évoluent dans le temps.\nLa preuve reste stable.',
      paragraph4: 'Horizon ne remplace jamais le jugement humain.\nIl garantit que ce jugement peut s\'appuyer sur des faits.',
      tagline: 'Les signaux suggèrent. La preuve établit.',
    },
    notIs: {
      title: 'Ce que Horizon n\'est pas',
      paragraph1: 'Pour éviter toute ambiguïté, Horizon n\'est pas :',
      list: [
        'une plateforme de monitoring',
        'un évaluateur de performance ou de qualité',
        'une interface d\'explicabilité',
        'un contrôle à l\'exécution / garde-fou',
        'un moteur de décision',
        'un tableau de bord de gouvernance',
      ],
      paragraph2: 'Horizon n\'intervient pas dans la génération.\nIl observe, préserve et atteste.',
      paragraph3: 'Horizon ne peut pas être reproduit par un enrichissement de logs, de l\'explicabilité, ou une reconstruction a posteriori.',
    },
    relationship: {
      title: 'Relation avec l\'existant',
      paragraph1: 'Les outils opérationnels apportent visibilité, alertes et contrôle pendant l\'exécution.\nHorizon ajoute une capacité différente.',
      paragraph2: 'Il garantit que ce qui se passe à l\'exécution pourra ensuite être examiné comme une preuve, sans dépendre de tableaux de bord, de configurations, ou d\'un accès au fournisseur.',
      tagline1: 'La visibilité est continue.',
      tagline2: 'La défendabilité est permanente.',
    },
    assurance: {
      title: 'Propriétés d\'assurance',
      paragraph1: 'L\'assurance produite via Horizon est conçue pour :',
      list: [
        'être au périmètre d\'une décision',
        'rester cohérente dans le temps',
        'être interprétable sans contexte propriétaire',
        'être vérifiable indépendamment, sans la plateforme Horizon',
        'être adaptée à l\'audit, au contrôle réglementaire ou au contentieux',
      ],
      paragraph2: 'L\'interprétation ne dépend ni d\'un réglage ni d\'un « tuning ».',
    },
    deployment: {
      title: 'Déploiement et périmètre de confiance',
      paragraph1: 'Horizon opère dans le périmètre de confiance de l\'organisation.\nL\'organisation conserve le contrôle :',
      list: [
        'de ses systèmes',
        'de ses données',
        'de ses éléments d\'assurance',
      ],
      paragraph2: 'Horizon ne requiert pas d\'accès continu pour interpréter ou valider les preuves.\nLa propriété de la preuve reste à l\'organisation.',
      paragraph3: 'Horizon ne déclenche pas de mesures correctives.\nIl permet une réponse humaine éclairée.',
    },
    users: {
      title: 'Utilisateurs visés',
      paragraph1: 'Horizon est conçu pour les équipes responsables de la supervision et de la redevabilité lorsque les décisions IA comptent :',
      list: [
        'Risque et Conformité',
        'Audit interne',
        'Gouvernance IA',
        'Équipes d\'ingénierie exploitant l\'IA à grande échelle',
      ],
      paragraph2: 'Horizon crée un socle factuel commun entre ces fonctions.\nIl est adapté aux environnements où les décisions peuvent être contestées.',
    },
    when: {
      title: 'Quand Horizon devient nécessaire',
      paragraph1: 'Horizon est utile avant que quoi que ce soit ne se passe mal.\nIl est utile au quotidien et critique lorsque le contrôle survient.',
      paragraph2: 'Il devient nécessaire lorsque :',
      list: [
        'des décisions IA peuvent être contestées',
        'un contrôle réglementaire ou juridique est probable',
        'l\'analyse post-incident doit s\'appuyer sur des faits',
        'les explications doivent rester défendables dans le temps',
      ],
      paragraph3: 'Si une décision compte, la preuve doit déjà exister.',
    },
    philosophy: {
      title: 'Philosophie',
      paragraph1: 'Horizon permet d\'adopter l\'IA sans dépendre de reconstructions fragiles ou de récits rétrospectifs.',
      tagline: 'Les signaux suggèrent. La preuve établit.',
    },
    cta: {
      title: 'Appel à l\'action',
      paragraph1: 'Découvrir comment Horizon s\'inscrit dans une stratégie d\'assurance IA centrée sur la responsabilité sous contrôle.',
      primaryCTA: 'Demander un briefing',
      secondaryCTA: 'Discuter des cas d\'usage audit et réglementaire',
    },
    contact: {
      title: 'Email',
      intro: 'Notre email:',
      copied: 'Copié',
      copy: "Copier l'email",
      mailto: 'Ouvrir dans mon client mail',
      note: 'Vous pouvez coller cet email dans votre messagerie.',
    },
  },
}

export async function getDictionary(locale: Locale) {
  return dictionaries[locale]
}
